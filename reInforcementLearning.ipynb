{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238ffd25",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5cbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re_inforcement means to train in live environment\n",
    "# Area 51 => Agent, Reward, Environment and Action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa80cc",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d9ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install gym \n",
    "# !pip install keras\n",
    "# !pip install keras_rl2\n",
    "# !pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d65adb",
   "metadata": {},
   "source": [
    "## Test Random Environment with OpenAI GYm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25bc802",
   "metadata": {},
   "outputs": [
   
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"States:\", states)\n",
    "print(\"Actions:\", actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7db6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for making a new Environment in the game\n",
    "episodes = 10 # Steps\n",
    "for episodes in range (1, episodes+1 ):\n",
    "    state = env.reset() # Resitting Environment \n",
    "    done = False\n",
    "    scores = 0\n",
    "    while not done:\n",
    "            env.render() \n",
    "            action = random.choice([0,1]) # Steps(either left or right side)\n",
    "            n_state, reward, done,info = env.step(action) # Step function generetes 5 varaibles\n",
    "            scores += reward # Increment scores with +1\n",
    "    print(\"Episode: {} Scores: {}\" .format(episodes, scores))\n",
    "print(n_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eae312",
   "metadata": {},
   "source": [
    "## Creating a Deep Learning model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # Deep Learning MOdel\n",
    "from tensorflow.keras.layers import Dense, Flatten # Layers\n",
    "from tensorflow.keras.optimizers import Adam # Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CReating function to reused it in future work \n",
    "def Build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = (1,states))) # First layer/state of the model\n",
    "    model.add(Dense(24, activation='relu')) # Second layer/state of the model\n",
    "    model.add(Dense(24, activation='relu')) # Third layer/state of the model\n",
    "#     model.add(Flatten()) \n",
    "    model.add(Dense(actions, activation='linear')) # Last layer/state of the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "print(states)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40984fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_array = np.random.random((1)\n",
    "model = Build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ceb97",
   "metadata": {},
   "source": [
    "## Building Agent with Keras RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit = 50000, window_length = 1)\n",
    "    dqna = DQNAgent( model=model , policy= policy, memory= memory,\n",
    "                   nb_actions = actions, nb_steps_warmup = 10, target_model_update = 1e-2 )\n",
    "    \n",
    "    return dqna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Build_agent(model,actions)\n",
    "agent.compile(Adam(learning_rate =1e-3), metrics=['mae'])\n",
    "agent.fit(env,  nb_steps= 50000,verbose=1, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dd777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "db9ccd9705adcf30ffb492caf920562b99a667ca56093ce5d48cedbc9ce03977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
